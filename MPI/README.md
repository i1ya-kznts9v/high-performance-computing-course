# Лабораторная работа №2

## Задание №A-06

Написать программу вычисления матричного выражения:

$A = Tr(B^3 C) E + I + B \\&\\& M$, где
- $Tr(...)$ - след матрицы, сумма ее диагональных элементов;
- $\\&\\&$ - поэлементное логическое «И».
- $B$, $C$ – квадратные плотные матрицы, элементы которых имеют тип `double`, их элементы задаются с помощью генератора псевдослучайных чисел;
- $E$ – полностью заполненная матрица, все элементы которой равны $1$;
- $I$ – единичная матрица, все диагональные элементы которой равны $1$;
- $M$ – матрица, элементы которой принимают значения $0$ или $1$ и задаются произвольным образом;

Необходимо распараллелить эту программу с помощью MPI. Исследовать зависимость масштабируемости параллельной версии программы от ее вычислительной трудоемкости (размера матриц). Проверить корректность параллельной версии. Проверить закона Амдала. Построить зависимость ускорения от числа процессов для заданного примера.

## Программно-аппаратная конфигурация

```
System:
  Distro: Windows 10 Pro 22H2
  Bits: 64
  Compiler: GCC 14.1.0
  MPI: MSMPI 8.0.1
  Interpreter: Python 3.13.0rc2
```

```
CPU:
  Model: Intel Core i7-11370H
  Architecture: Tiger Lake H35
  Cores: 4-core
  Bits: 64
  Speed (MHz):
    Min: 800
    Max: 4800
  Cache:
    L1: 80 KiB
    L2: 1.25 MB MiB
    L3: 12 MiB
  Flags: 
    avx avx2 ht lm nx pae
    sse sse2 sse3 sse4_1
    sse4_2 ssse3 vmx
```

```
RAM:
  Total: 15.8 GiB
```

## Команды

Компиляция и запуск вычисления матричного выражения с указанным числом процессов и размерностью матриц. Результатом являются данные о времени матричного вычисления для указанных аргументов в формате CSV.

```
g++ -I [path to /MPI/Lib/x64] -I [path to /MPI/Include/x64] -I [path to /MPI/Include] -L . -lmsmpi -g matrix_computation.cpp -o matrix_computation.exe
mpiexec -n [processes] ./matrix_computation [dimension]
```

Запуск эксперимента, выполняющего компиляцию, вычисление матричного выражения, ускорения и эффективности, а также построение графиков времени, ускорения и эффективности по результатам вычисления в формате CSV.

```
python -m venv venv
./venv/Scripts/activate
pip install -r ./requirements.txt
python ./experiment.py
```

## Решение

### matrix_computation.cpp

В начале программы вызывается инициализация MPI `MPI_Init(&argc, &argv)`. Все пути завершения программы вызывают деинициализацию MPI `MPI_Finalize()`.

Перед началом матричных вычислений выполняется инициализация констант:
- `rank` - идентификатор текущего процесса ($0$ для основного, $1$ и более для порожденного);
- `size` - общее число процессов;
- `n` - размерность квадратной матрицы;
- `average = 5` - количество итераций для усреднения результатов.

Выполняется установка `seed` `srand(2001)` для воспроизводимой псевдослучайной генерации и инициализация числами `double` матриц из условия задачи в виде одномерных векторов, которую выполняет основной процесс:
- Матрицы $B$ и $C$ заполняются псевдослучайными числами, не умоляя общности, в отрезке $[0.0, 1.0]$;
- Матрица $E$ заполняется числами $1.0$;
- Матрица $I$ заполняется числами $1.0$ на диагонали;
- Матрица $M$ заполняется псевдослучайными числами $0.0$ и $1.0$.

Одномерность векторов необходима для корректной работы с буфером сообщений.

Инициализированные в основном процессе матрицы распространяются в порожденные процессы при помощи функции `MPI_Bcast()`. В итоге все процессы имеют одинаковые матрицы. Матрицы также переиспользуются между вычислениями.

В начале матричного вычисления определяются константы, необходимые для распараллеливания:
- `rows_per_process` - число строк матрицы, обрабатываемых каждым процессом;
- `start_row` - номер первой строки для выделения подматрицы для процесса, в зависимости от его `rank`; 
- `end_row` - номер последней строки для выделения подматрицы для процесса, в зависимости от его `rank`.

Затем создаются матрицы для хранения промежуточных результатов вычислений в каждом процессе.

Умножение матриц происходит методом «строка на столбец», но через умножение их подматриц, распределенных на каждый процесс по границам [`start_row`, `end_row`). Результат записывается промежуточную матрицу, что позволяет избежать зависимостей по данным.

После возведения $B^2$ каждый процесс содержит результат умножения подматриц. Поэтому для последующего возведения $B^3$ необходимо вычислить общий результат умножения матриц и синхронизировать его для всех процессов. Для этого используется функция `MPI_Allgather()`. `MPI_IN_PLACE` позволяет использовать один буфер для отправки и приема сообщений.

Вычисление $B^3 C$ не требует синхронизации через `MPI_Allgather()`, поскольку каждый процесс продолжает умножать свою подматрицу и не возникает зависимости по данным между процессами.

След матрицы $Tr(B^3 C)$ сначала вычисляется частично в каждом процессе, а затем с помощью `MPI_Allreduce()` вычисляется полностью и синхронизируется для всех процессов.

Следующим шагом происходит вычисление $Tr(B^3 C) E$ - умножение матрицы на константу, которое каждый процесс выполняет для своей подматрицы.

Затем каждым процессом вычисляется своя часть поэлементного логического «И» $B \\&\\& M$.

В заключении каждый процесс вычисляет свою часть результирующей матрицы $A$ через сумму раннее вычисленных подматриц $A = Tr(B^3 C) E + I + B \\&\\& M$. Общий результат вычисляется и синхронизируется между процессами с помощью `MPI_Allgather`.

Время вычисления замеряется в каждом процессе с помощью разности значений `chrono::high_resolution_clock::now()`, сохраненных до и после вычисления. Затем в основном процессе собирается общее время вычисления со всех процессов при помощи `MPI_Reduce()` и усредняется по их количеству. Сохраняется общее время всех итераций для последующего усреднения.

Перед завершением программы вычисляется среднее время, и результаты времени матричных вычислений сохраняются в формате CSV для дальнейшего представления и интерпретации.

### experiment.py

Для подсчета статистики, представления результатов в виде графиков и их интерпретации используется скрипт, который запускает матричные вычисления и получает результаты времени матричных вычислений в формате CSV. Для построения графиков используются `pandas` и `matplotlib`.

Сперва происходит запуск быстрого эксперимента с размерностью матриц $500$, чтобы проверить, что все работает. Затем эксперимент выполняется для матриц размерностью $1000$ и $2000$, что занимает ощутимо больше времени.

Для матриц указанной размерности происходит запуск вычислений с увеличением числа процессов по степеням числа $2$, максимальная степень указана $8$ (не включительно). По завершении эксперимента результаты времени матричных вычислений доступны в формате CSV. 

На их основе происходит подсчет статистики матричных вычислений. Вычисляются коэффициенты ускорения и эффективности исходя из полученных значений и законов Амдала, которые определяют предельные значения.
Размер параллельной части программы для законов Амдала был оценен как $0.9$. Статистика также сохраняется в формате CSV.

В ходе каждого эксперимента строится график времени, ускорения и эффективности матричных вычислений в сравнении с предельными значениями из законов Амдала.
На заключительном шаге строится график масштабируемости для матриц разной размерности.

## Результаты

### Размерность матриц $1000$

Таблица $1$ времени матричных вычислений для разного числа процессов. В таблице содержится время каждого из пяти запусков и среднее время для каждого числа процессов. Данные для $128$ процессов отсутствуют в таблице, поскольку системе Windows не удалось создать столько процессов для указанной программно-аппаратной конфигурации.

| Процессы | 1 | 2 | 3 | 4 | 5 | Среднее (сек.) |
| --- | --- | --- | --- | --- | --- | --- |
| 1 | 18.07 | 17.5 | 17.66 | 17.74 | 17.76 | 17.75 |
| 2 | 11 | 11.22 | 11.22 | 11.01 | 11.14 | 11.12 |
| 4 | 7.5 | 7.57 | 7.42 | 7.55 | 7.5 | 7.51 |
| 8 | 6.69 | 6.55 | 6.52 | 6.64 | 6.6 | 6.6 |
| 16 | 6.8 | 6.69 | 6.71 | 6.83 | 6.7 | 6.75 |
| 32 | 8.4 | 7.97 | 8.05 | 7.67 | 7.71 | 7.96 |
| 64 | 8.82 | 9.01 | 9.11 | 8.97 | 8.77 | 8.94 |

По табличным данным был построен график $1$: зависимости времени от числа процессов.

![График 1](./results/times_1000.png)

Таблица $2$ коэффициентов ускорения и эффективности для разного числа процессов в сравнении с предельными значениями из законов Амдала.

| Процессы | Ускорение Амдала | Ускорение | Эффективность Амдала | Эффективность |
| --- | --- | --- | --- | --- |
| 1 | 1 | 1 | 1 | 1 |
| 2 | 1.82 | 1.6 | 0.91 | 0.8 |
| 4 | 3.08 | 2.36 | 0.77 | 0.59 |
| 8 | 4.71 | 2.69 | 0.59 | 0.34 |
| 16 | 6.4 | 2.63 | 0.4 | 0.16 |
| 32 | 7.8 | 2.23 | 0.24 | 0.07 |
| 64 | 8.77 | 1.99 | 0.14 | 0.03 |

По табличным данным был построен график $2$: зависимости ускорения от числа процессов.

![График 2](./results/speedup_1000.png)

По табличным данным был построен график $3$: зависимости эффективности от числа процессов.

![График 3](./results/efficency_1000.png)

### Размерность матриц $2000$

Таблица $3$ времени матричных вычислений для разного числа процессов. В таблице содержится время каждого из пяти запусков и среднее время для каждого числа процессов. Данные для $128$ процессов отсутствуют в таблице, поскольку системе Windows не удалось создать столько процессов для указанной программно-аппаратной конфигурации.

| Процессы | 1 | 2 | 3 | 4 | 5 | Среднее (сек.) |
| --- | --- | --- | --- | --- | --- | --- |
| 1 | 162.03 | 160.07 | 159.64 | 160.2 | 159.39 | 160.27 |
| 2 | 95.21 | 94.54 | 94.74 | 92.83 | 91.9 | 93.84 |
| 4 | 61.95 | 61.91 | 61.91 | 62.24 | 61.86 | 61.98 |
| 8 | 55.32 | 55.41 | 55.39 | 55.47 | 55.66 | 55.45 |
| 16 | 56.47 | 56.16 | 56.22 | 55.96 | 56.15 | 56.19 |
| 32 | 62.24 | 59.04 | 58.4 | 58.36 | 59.08 | 59.42 |
| 64 | 77.33 | 67.52 | 67.38 | 67.43 | 66.3 | 69.19 |

По табличным данным был построен график $4$: зависимости времени от числа процессов.

![График 4](./results/times_2000.png)

Таблица $4$ коэффициентов ускорения и эффективности для разного числа процессов в сравнении с предельными значениями из законов Амдала.

| Процессы | Ускорение Амдала | Ускорение | Эффективность Амдала | Эффективность |
| --- | --- | --- | --- | --- |
| 1 | 1 | 1 | 1 | 1 |
| 2 | 1.82 | 1.71 | 0.91 | 0.85 |
| 4 | 3.08 | 2.59 | 0.77 | 0.65 |
| 8 | 4.71 | 2.89 | 0.59 | 0.36 |
| 16 | 6.4 | 2.85 | 0.4 | 0.18 |
| 32 | 7.8 | 2.7 | 0.24 | 0.08 |
| 64 | 8.77 | 2.32 | 0.14 | 0.04 |

По табличным данным был построен график $5$: зависимости ускорения от числа процессов.

![График 5](./results/speedup_2000.png)

По табличным данным был построен график $6$: зависимости эффективности от числа процессов.

![График 6](./results/efficency_2000.png)

### Масштабируемость

Для оценки масштабируемости в зависимости от размерности матриц был построен график $7$. На нем сравниваются графики ускорения для приведенных размерностей матриц.

![График 7](./results/scalability.png)

## Выводы

Построенные графики зависимости ускорения и эффективности от числа процессов демонстрируют схожую тенденцию с законом Амдала. При небольшом числе процессов оба графика практически полностью совпадают, однако с увеличением числа процессов они начинают расходиться - снижается и ускорение, и эффективность. При большом числе процессов снова происходит увеличение времени матричных вычислений. Это подтверждает, что при большом числе процессов значительную роль играют накладные расходы на создание, обмен сообщениями, синхронизацию и остановку процессов, а количество выполненной полезной работы для каждого процесса снижается.

По построенному графику масштабируемости для размерностей матриц $1000$ и $2000$ ясно, что чем выше трудоемкость задачи, тем больше масштабируются её вычисления, а значит может быть достигнуто большее ускорение и эффективность вычислений.
